# src/news_pipeline.py
"""Complete news processing pipeline with real user integration, feedback, and data retention.
Implements NEW optimized architecture:
1. Background task fetches/classifies/generates YNK for ALL news ONCE and saves to DB (news_items).
2. Background task generates personalized TOP-7 for ALL users ONCE from pre-processed news and caches it (user_news_cache).
3. Background task generates personalized podcast scripts for PREMIUM users ONCE and caches it (user_news_cache).
4. API endpoint only reads pre-built bundle from cache for instant response.
"""

import asyncio
import os
import sys
import time
from collections import defaultdict
from datetime import date, datetime, timedelta
from typing import Any, Dict, List, Optional, Set, Union


# --- Path setup for internal imports ---
def setup_paths():
    """Setup paths for correct imports."""
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    paths_to_add = [project_root, current_dir]
    for path in paths_to_add:
        if path not in sys.path:
            sys.path.insert(0, path)


setup_paths()

# Import our modules
try:
    from news_fetcher import SmartNewsFetcher

    print("‚úÖ Imported SmartNewsFetcher successfully")
except ImportError as e:
    print(f"‚ùå Failed to import SmartNewsFetcher: {e}")
    sys.exit(1)

try:
    from user_profile import USER_PROFILES, get_user_profile

    print("‚úÖ Imported user_profile successfully")
except ImportError as e:
    print(f"‚ùå Failed to import user_profile: {e}")
    sys.exit(1)

try:
    from cache_manager import get_cache_manager

    print("‚úÖ Imported cache_manager successfully")
except ImportError as e:
    print(f"‚ùå Failed to import cache_manager: {e}")
    sys.exit(1)

try:
    from summarizer import summarize_news

    print("‚úÖ Imported summarizer successfully")
except ImportError as e:
    print(f"‚ùå Failed to import summarizer: {e}")
    summarize_news = None

try:
    from feedback_system import feedback_system

    print("‚úÖ Imported feedback_system successfully")
except ImportError as e:
    print(f"‚ùå Failed to import feedback_system: {e}")
    feedback_system = None

# --- Import Data Retention Module ---
try:
    from data_retention import perform_data_retention_cleanup

    DATA_RETENTION_ENABLED = True
    print("‚úÖ Imported data_retention successfully")
except ImportError as e:
    print(f"‚ö†Ô∏è Data retention module not available: {e}. Cleanup will be skipped.")
    perform_data_retention_cleanup = None
    DATA_RETENTION_ENABLED = False

# Import database session and models for saving news items and caching bundles
try:
    from sqlalchemy import delete, select

    from database import AsyncSessionFactory  # Correct import
    from models import NewsItem
    from models import User as DBUser  # SQLAlchemy models
    from models import UserNewsCache
    from models import UserProfile as DBUserProfile

    DATABASE_AVAILABLE = True
    print("‚úÖ Imported database and models successfully")
except ImportError as e:
    print(f"‚ö†Ô∏è Database or models not fully available for saving news items: {e}")
    AsyncSessionFactory = None
    NewsItem = None
    DBUser = None
    DBUserProfile = None
    UserNewsCache = None
    select = None
    delete = None
    DATABASE_AVAILABLE = False

# --- Import Podcast Generator ---
try:
    from src.podcast_generator import get_podcast_generator  # <-- –ò–º–ø–æ—Ä—Ç

    PODCAST_GENERATOR_AVAILABLE = True
    print("‚úÖ Imported podcast_generator successfully")
except ImportError as e:
    print(f"‚ö†Ô∏è Podcast generator module not available: {e}. Podcasts will be skipped.")
    get_podcast_generator = None
    PODCAST_GENERATOR_AVAILABLE = False


class NewsProcessingPipeline:
    """Orchestrates the complete news processing workflow for personalized delivery."""

    def __init__(self, max_workers: int = 5):
        """
        Initialize the complete news processing pipeline.

        Args:
            max_workers: Maximum number of concurrent tasks
        """
        self.max_workers = max_workers
        self.fetcher = SmartNewsFetcher()  # Use the enhanced fetcher
        self.cache = get_cache_manager()
        self.feedback_system = feedback_system
        self.summarize_news_func = summarize_news  # Store the summarizer function
        self.podcast_generator = get_podcast_generator()  # <-- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        self.processed_news_count = 0
        self.total_processing_time = 0.0
        print(
            "üöÄ NewsProcessingPipeline initialized with enhanced SmartNewsFetcher and PodcastGenerator"
        )

    def get_all_users(self) -> List[Any]:
        """Get all registered users from the system."""
        users = []
        for user_id in USER_PROFILES.keys():
            user = get_user_profile(user_id)
            if user:
                users.append(user)
        print(f"üë• Loaded {len(users)} users from system")
        return users

    async def get_all_users_from_db(self) -> List[Dict[str, Any]]:
        """Get all registered users from the database."""
        if (
            not DATABASE_AVAILABLE
            or not AsyncSessionFactory
            or not DBUser
            or not DBUserProfile
        ):
            print(
                "‚ö†Ô∏è Database not configured for fetching users. Returning system users."
            )
            # Fallback to system users if DB is not available
            system_users = self.get_all_users()
            db_user_list = []
            for user in system_users:
                user_dict = self._convert_user_profile_to_dict(user)
                db_user_list.append(
                    {
                        "id": user_dict.get("user_id"),
                        "email": f"{user_dict.get('user_id')}@example.com",  # Fallback email
                        "profile": user_dict,
                    }
                )
            return db_user_list

        async with AsyncSessionFactory() as db_session:
            try:
                # Join User and UserProfile
                stmt = select(DBUser, DBUserProfile).join(DBUserProfile, isouter=True)
                result = await db_session.execute(stmt)
                db_users_with_profiles = result.all()

                users_list = []
                for db_user, db_profile in db_users_with_profiles:
                    user_data = {
                        "id": db_user.id,
                        "email": db_user.email,
                        "profile": {
                            "user_id": db_user.id,
                            "locale": db_profile.locale if db_profile else "US",
                            "language": "en",  # Default or from profile
                            "city": None,  # Not stored
                            "interests": db_profile.interests if db_profile else [],
                        }
                        if db_profile
                        else None,
                    }
                    users_list.append(user_data)

                print(f"üë• Loaded {len(users_list)} users from database")
                return users_list
            except Exception as e:
                print(f"‚ö†Ô∏è Error fetching users from DB: {e}")
                return []

    def _convert_user_profile_to_dict(self, user_profile: Any) -> Dict[str, Any]:
        """
        Converts a user profile (object or dict) to a standard dictionary.
        Ensures compatibility with functions expecting a dict.
        """
        if isinstance(user_profile, dict):
            return user_profile
        elif hasattr(user_profile, "__dict__"):
            return user_profile.__dict__
        else:
            # Fallback for unexpected types (e.g., SimpleNamespace)
            print(
                f"‚ö†Ô∏è Unexpected user profile type: {type(user_profile)}. Attempting attribute access."
            )
            try:
                return {
                    "user_id": getattr(user_profile, "user_id", "unknown"),
                    "locale": getattr(user_profile, "locale", "US"),
                    "language": getattr(user_profile, "language", "en"),
                    "city": getattr(user_profile, "city", ""),
                    "interests": getattr(user_profile, "interests", []),
                }
            except Exception as e:
                print(f"‚ùå Failed to convert user profile to dict: {e}")
                return {}

    async def _save_news_items_to_db(self, news_bundle: dict):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ news_bundle –≤ –ë–î –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –∏—Ö id."""
        if not DATABASE_AVAILABLE or not AsyncSessionFactory or not NewsItem:
            print("‚ö†Ô∏è Database not configured for saving news items. Skipping.")
            return

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∏–∑ news_bundle
        all_articles = []
        for category_articles in news_bundle.values():
            if isinstance(category_articles, list):
                all_articles.extend(category_articles)

        # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ URL'—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        unique_urls = set()
        articles_to_process = []
        for article in all_articles:
            url = article.get("url")
            if url and url not in unique_urls:
                unique_urls.add(url)
                articles_to_process.append(article)

        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–µ—Å—Å–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π
        async with AsyncSessionFactory() as db_session:
            try:
                for article in articles_to_process:
                    # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –Ω–æ–≤–æ—Å—Ç—å –ø–æ URL
                    stmt = select(NewsItem).where(NewsItem.url == article["url"])
                    result = await db_session.execute(stmt)
                    existing_item = result.scalar_one_or_none()

                    if existing_item:
                        # –ù–æ–≤–æ—Å—Ç—å —É–∂–µ –µ—Å—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë ID
                        article["id"] = existing_item.id

                        # --- –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ ai_analysis ---
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–∏—Ç—å ai_analysis
                        # –£—Å–ª–æ–≤–∏–µ: ai_analysis –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–µ–º –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á–µ–π
                        needs_ai_update = (
                            not existing_item.ai_analysis
                            or not isinstance(existing_item.ai_analysis, dict)
                            or "ynk_summary" not in existing_item.ai_analysis
                            or not existing_item.ai_analysis.get("ynk_summary")
                            or "relevance_score" not in existing_item.ai_analysis
                            or "confidence" not in existing_item.ai_analysis
                        )

                        if needs_ai_update:
                            print(
                                f"  üîÑ Updating incomplete ai_analysis for existing item ID {existing_item.id}..."
                            )

                            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ article —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ
                            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º YNK, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç –≤ article
                            if (
                                "ynk_summary" not in article
                                or not article["ynk_summary"]
                            ):
                                ynk_summary = self._generate_ynk_summary(article)
                                article["ynk_summary"] = ynk_summary
                            else:
                                ynk_summary = article["ynk_summary"]

                            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ article —Å–æ–¥–µ—Ä–∂–∏—Ç scores (–æ–Ω–∏ –æ–±—ã—á–Ω–æ –ø—Ä–∏—Ö–æ–¥—è—Ç –∏–∑ fetcher/classifier)
                            relevance_score = article.get("relevance_score", 0)
                            confidence = article.get("confidence", 0)

                            # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—ä–µ–∫—Ç SQLAlchemy
                            existing_item.ai_analysis = {
                                "relevance_score": relevance_score,
                                "confidence": confidence,
                                "ynk_summary": ynk_summary,
                            }

                            # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–µ—Å—Å–∏—é –¥–ª—è –∫–æ–º–º–∏—Ç–∞
                            db_session.add(existing_item)
                            await db_session.commit()
                            await db_session.refresh(existing_item)
                            print(
                                f"  ‚úÖ Updated ai_analysis for item ID {existing_item.id}."
                            )
                        # --- –ö–û–ù–ï–¶ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ì–û –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø ---

                    else:
                        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –Ω–æ–≤–æ—Å—Ç—å
                        # --- –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –û–±–µ—Å–ø–µ—á–∏—Ç—å, —á—Ç–æ external_id –Ω–µ None ---
                        external_id_from_article = article.get(
                            "external_id"
                        )  # –ú–æ–∂–µ—Ç –±—ã—Ç—å None
                        external_id_to_use = (
                            external_id_from_article
                            if external_id_from_article is not None
                            else article["url"]
                        )

                        # --- –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï 1: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ YNK —É–∂–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ article ---
                        # –ü—Ä–æ–≤–µ—Ä–∏–º, –µ—Å—Ç—å –ª–∏ YNK –≤ article. –ï—Å–ª–∏ –Ω–µ—Ç, —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º.
                        # –≠—Ç–æ –≤–∞–∂–Ω–æ, –µ—Å–ª–∏ _save_news_items_to_db –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –Ω–µ –∏–∑ fetch_and_classify_all_news –Ω–∞–ø—Ä—è–º—É—é.
                        if "ynk_summary" not in article or not article["ynk_summary"]:
                            ynk_summary = self._generate_ynk_summary(article)
                            article[
                                "ynk_summary"
                            ] = ynk_summary  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ article –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                        else:
                            ynk_summary = article[
                                "ynk_summary"
                            ]  # –ë–µ—Ä–µ–º —É–∂–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π

                        new_item = NewsItem(
                            external_id=external_id_to_use,  # <-- –ò–°–ü–†–ê–í–õ–ï–ù–û
                            source_name=article.get(
                                "source_name", article.get("source", "Unknown")
                            ),
                            title=article["title"],
                            url=article["url"],
                            category=article.get("category", "unknown"),
                            subcategory=article.get("subcategory"),  # –ú–æ–∂–µ—Ç –±—ã—Ç—å None
                            importance_score=article.get("importance_score", 0),
                            ai_analysis={
                                "relevance_score": article.get("relevance_score", 0),
                                "confidence": article.get("confidence", 0),
                                # –î–æ–±–∞–≤–ª—è–µ–º YNK –≤ ai_analysis –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î
                                "ynk_summary": ynk_summary,
                            },
                            fetched_at=datetime.utcnow(),
                            # ynk_summary –±—É–¥–µ—Ç —Ö—Ä–∞–Ω–∏—Ç—å—Å—è –≤ ai_analysis, –∫–∞–∫ —É–∫–∞–∑–∞–Ω–æ –≤—ã—à–µ.
                        )
                        # –£–¥–∞–ª–µ–Ω–∞ –¥—É–±–ª–∏—Ä—É—é—â–∞—è –∑–∞–ø–∏—Å—å ynk_summary –≤ ai_analysis, –æ–Ω–∞ —É–∂–µ —Ç–∞–º –≤—ã—à–µ.

                        db_session.add(new_item)
                        await db_session.commit()  # –°–æ—Ö—Ä–∞–Ω—è–µ–º
                        await db_session.refresh(new_item)  # –ü–æ–ª—É—á–∞–µ–º ID
                        article["id"] = new_item.id
                        # print(f"Created new news item: {new_item.id}") # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏

                print(
                    f"‚úÖ Saved/Checked {len(articles_to_process)} unique news items to DB."
                )
            except Exception as e:
                print(f"‚ö†Ô∏è Error saving news items to DB: {e}")
                await db_session.rollback()
                # –ú–æ–∂–Ω–æ –≤—ã–±—Ä–æ—Å–∏—Ç—å –æ—à–∏–±–∫—É –∏–ª–∏ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å (–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ!)
                # raise

    def _generate_ynk_summary(self, article: Dict) -> str:
        """Generates a YNK (Why Not Care) summary for an article."""
        if not self.summarize_news_func:
            return "Summary generation module (summarizer.py) not available."

        try:
            # Use content, description, or title
            news_text = (
                article.get("content", "")
                or article.get("description", "")
                or article.get("title", "")
            )
            if not news_text.strip():
                return "No content, description, or title available for summary."

            # Use the AI-classified category to select the correct impact aspects
            category = article.get("category", "general")

            # Call summarize_news from summarizer.py
            summary = self.summarize_news_func(news_text, category)
            return summary
        except Exception as e:
            # print(f"‚ö†Ô∏è YNK summary generation failed for '{article.get('title', 'Unknown')}': {e}") # Minimize logs
            return f"Could not generate summary. Error: {e}"

    def _select_top_articles_for_user(
        self, classified_news_list: List[Dict], user_profile: Union[Dict, Any]
    ) -> List[Dict]:
        """
        Selects the TOP-7 articles for a specific user from the ALREADY CLASSIFIED news list.
        Guarantees representation from specific subcategories IF their relevance is high enough.
        Otherwise, fills TOP-7 with the best overall articles.
        """
        # --- CRITICAL FIX: Convert user_profile to dict ---
        user_profile_dict = self._convert_user_profile_to_dict(user_profile)
        # --- END FIX ---

        selected_articles = []
        seen_titles: Set[str] = set()  # For deduplication within TOP-7

        user_interests = user_profile_dict.get("interests", [])

        # --- Improved Interest Extraction Logic ---
        # 1. Extract main categories (e.g., 'economy_finance', 'technology_ai_science')
        main_categories = [
            interest for interest in user_interests if isinstance(interest, str)
        ]

        # 2. Extract specific subcategories (e.g., 'basketball_nba', 'football_epl')
        specific_subcategories: Set[str] = set()
        for interest in user_interests:
            if isinstance(interest, dict):
                for main_cat, subcats in interest.items():
                    if isinstance(subcats, list):
                        specific_subcategories.update(subcats)

        # --- Selection Logic with Minimum Relevance Threshold ---
        MIN_RELEVANCE_THRESHOLD = 0.40  # Articles below this won't be forced in

        # 1. Try to guarantee articles for each specific subcategory IF they meet the threshold
        sorted_specific_subcats = list(specific_subcategories)
        if self.feedback_system:
            user_id = user_profile_dict.get("user_id", "unknown_user")
            sorted_specific_subcats.sort(
                key=lambda subcat: self.feedback_system.get_user_preference(
                    user_id, subcat
                ),
                reverse=True,
            )

        for subcategory in sorted_specific_subcats:
            # Find articles matching the specific subcategory field
            matching_articles = []
            for article in classified_news_list:  # <-- –ò—Å–ø–æ–ª—å–∑—É–µ–º classified_news_list
                # Check for subcategory fields like 'sports_subcategory', 'economy_subcategory'
                article_subcats = [
                    article.get("sports_subcategory"),
                    article.get("economy_subcategory"),
                    article.get("tech_subcategory")
                    # Add others if classifier provides them
                ]
                if subcategory in article_subcats:
                    matching_articles.append(article)

            # Sort by relevance and pick the best unique one that meets the threshold
            matching_articles.sort(
                key=lambda x: x.get("relevance_score", 0), reverse=True
            )
            article_added_for_this_subcat = False
            for article in matching_articles:
                title_key = article.get("title", "").lower()
                relevance = article.get("relevance_score", 0)
                if (
                    title_key not in seen_titles
                    and len(selected_articles) < 7
                    and not article_added_for_this_subcat
                    and relevance >= MIN_RELEVANCE_THRESHOLD
                ):  # <-- NEW CHECK
                    selected_articles.append(article)
                    seen_titles.add(title_key)
                    # print(f"üìå Guaranteed article for specific subcategory '{subcategory}' (Score: {relevance:.2f}): {article.get('title', 'No Title')[:50]}...") # Minimize logs
                    article_added_for_this_subcat = True  # Take only the first (best) unique article from this subcategory
                    break  # Break inner loop once we find a good match

        # 2. Guarantee articles for main categories (that don't have specific subcats defined or weren't satisfied) IF they meet threshold
        sorted_main_cats = list(main_categories)
        if self.feedback_system:
            user_id = user_profile_dict.get("user_id", "unknown_user")
            sorted_main_cats.sort(
                key=lambda cat: self.feedback_system.get_user_preference(user_id, cat),
                reverse=True,
            )

        for category in sorted_main_cats:
            # Only add if we haven't filled the quota AND article meets threshold
            if len(selected_articles) >= 7:
                break

            category_articles = [
                a for a in classified_news_list if a.get("category") == category
            ]  # <-- –ò—Å–ø–æ–ª—å–∑—É–µ–º classified_news_list
            category_articles.sort(
                key=lambda x: x.get("relevance_score", 0), reverse=True
            )

            article_added_for_this_cat = False
            for article in category_articles:
                title_key = article.get("title", "").lower()
                relevance = article.get("relevance_score", 0)
                if (
                    title_key not in seen_titles
                    and len(selected_articles) < 7
                    and not article_added_for_this_cat
                    and relevance >= MIN_RELEVANCE_THRESHOLD
                ):  # <-- NEW CHECK
                    selected_articles.append(article)
                    seen_titles.add(title_key)
                    # print(f"üìå Guaranteed article for main category '{category}' (Score: {relevance:.2f}): {article.get('title', 'No Title')[:50]}...") # Minimize logs
                    article_added_for_this_cat = True  # Take only the first (best) unique article from this main category
                    break  # Break inner loop once we find a good match

        # 3. Fill remaining slots with the BEST articles overall (no threshold for fillers)
        if len(selected_articles) < 7:
            all_articles_sorted = sorted(
                [
                    a
                    for cat_articles in classified_news_list.values()
                    for a in cat_articles
                ],  # Flatten all articles
                key=lambda x: x.get("relevance_score", 0),
                reverse=True,
            )
            for article in all_articles_sorted:
                if len(selected_articles) >= 7:
                    break
                title_key = article.get("title", "").lower()
                if title_key not in seen_titles:
                    selected_articles.append(article)
                    seen_titles.add(title_key)
                    # print(f"üîù Added to TOP-7 (filler): {article.get('title', 'No Title')[:50]}... (Score: {article.get('relevance_score', 0):.2f})") # Minimize logs

        # Ensure maximum of 7 and sort by final relevance score for display
        final_selection = selected_articles[:7]
        final_selection.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)

        print(
            f"üéØ Final news bundle ready: {len(final_selection)} articles selected for TOP-7"
        )
        # Optional: Print breakdown by category
        # from collections import Counter
        # cats = [a.get('category', 'unknown').upper() for a in final_selection]
        # cat_counts = Counter(cats)
        # for cat, count in cat_counts.items():
        #     print(f"   {cat}: {count} articles")

        return final_selection

    async def _run_data_retention_cleanup(self):
        """Runs the data retention cleanup tasks asynchronously."""
        if (
            not DATA_RETENTION_ENABLED
            or not DATABASE_AVAILABLE
            or not AsyncSessionFactory
        ):
            print(
                "‚ö†Ô∏è Data retention is disabled or database is not configured. Skipping cleanup."
            )
            return

        print("--- Initiating Automatic Data Retention Cleanup ---")
        try:
            # Use the imported AsyncSessionFactory
            async with AsyncSessionFactory() as session:
                # Call the function from data_retention.py
                deleted_counts = await perform_data_retention_cleanup(session)
                print("--- Data Retention Cleanup Summary ---")
                for task, count in deleted_counts.items():
                    print(f"  ‚úÖ {task}: {count} items")
                print("--------------------------------------")
        except Exception as e:
            print(f"‚ö†Ô∏è Data retention cleanup failed: {e}")

    # --- –ù–û–í–´–ï –ú–ï–¢–û–î–´ –î–õ–Ø –ù–û–í–û–ô –ê–†–•–ò–¢–ï–ö–¢–£–†–´ ---

    async def fetch_and_classify_all_news(self) -> List[Dict]:
        """
        –§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ 1: –°–æ–±–∏—Ä–∞–µ—Ç, –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –í–°–ï –Ω–æ–≤–æ—Å—Ç–∏.
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π YNK –¥–ª—è –∫–∞–∂–¥–æ–π —É–Ω–∏–∫–∞–ª—å–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–æ–≤–æ—Å—Ç–µ–π (–≤–∫–ª—é—á–∞—è ID –∏–∑ –ë–î).
        """
        print("\n--- [BACKGROUND TASK 1] Fetching and Classifying ALL News ---")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å –¥–ª—è —Å–±–æ—Ä–∞ "–≤—Å–µ–≥–æ"
        dummy_profile = {
            "user_id": "background_processor",
            "locale": "US",
            "language": "en",
            "interests": [
                "economy_finance",
                "technology_ai_science",
                "politics_geopolitics",
                "healthcare_pharma",
                "culture_media_entertainment",
                "sports",
                "transport_auto_aviation",
                "lifestyle_travel_tourism",
            ],  # –í—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –º–∞–∫—Å. –æ—Ö–≤–∞—Ç–∞
        }

        start_time = time.time()
        print(f"üì° Fetching global news bundle for all categories...")

        # --- 1. Fetch all news ---
        news_bundle = self.fetcher.fetch_daily_news_bundle(dummy_profile)
        fetch_time = time.time() - start_time
        print(
            f"üì¶ Raw articles collected: {sum(len(arts) for arts in news_bundle.values())} (in {fetch_time:.2f}s)"
        )

        # --- 2. Save to DB (includes classification and YNK generation) ---
        await self._save_news_items_to_db(news_bundle)
        save_time = time.time() - start_time - fetch_time
        print(f"üíæ News saved/classified/YNK'd to DB (in {save_time:.2f}s)")

        # --- 3. Fetch the saved news back with IDs ---
        # –≠—Ç–æ –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å ID –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞
        if not DATABASE_AVAILABLE or not AsyncSessionFactory or not NewsItem:
            print("‚ö†Ô∏è Cannot fetch saved news from DB. Returning empty list.")
            return []

        async with AsyncSessionFactory() as db_session:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—É—Ç–∫–∏ (–∏–ª–∏ –∑–∞ –¥—Ä—É–≥–æ–µ –æ–∫–Ω–æ)
                yesterday = datetime.utcnow() - timedelta(days=1)
                stmt = select(NewsItem).where(NewsItem.fetched_at >= yesterday)
                result = await db_session.execute(stmt)
                saved_news_items = result.scalars().all()

                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º SQLAlchemy –º–æ–¥–µ–ª–∏ –≤ —Å–ª–æ–≤–∞—Ä–∏
                saved_news_list = []
                for item in saved_news_items:
                    item_dict = {
                        "id": item.id,
                        "external_id": item.external_id,
                        "source_name": item.source_name,
                        "title": item.title,
                        "url": item.url,
                        "category": item.category,
                        "subcategory": item.subcategory,
                        "importance_score": item.importance_score,
                        "ai_analysis": item.ai_analysis,  # –°–æ–¥–µ—Ä–∂–∏—Ç relevance_score, confidence, ynk_summary
                        "fetched_at": item.fetched_at.isoformat()
                        if item.fetched_at
                        else None,
                    }
                    saved_news_list.append(item_dict)

                print(
                    f"üì§ Fetched {len(saved_news_list)} classified news items with IDs from DB."
                )
                return saved_news_list
            except Exception as e:
                print(f"‚ö†Ô∏è Error fetching saved news from DB: {e}")
                return []

    async def generate_and_cache_bundles_for_all_users(
        self, classified_news_list: List[Dict]
    ):
        """
        –§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ 2: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ª–µ–Ω—Ç—ã –¥–ª—è –í–°–ï–• –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –≤ –∫—ç—à (user_news_cache).
        –ò–°–ü–û–õ–¨–ó–£–ï–¢ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ classified_news_list.

        Args:
            classified_news_list: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–æ–≤–æ—Å—Ç–µ–π (–≤–∫–ª—é—á–∞—è ID –∏ YNK).
        """
        print(
            "\n--- [BACKGROUND TASK 2] Generating Personalized Bundles for ALL Users ---"
        )

        if not classified_news_list:
            print("‚ö†Ô∏è No classified news provided. Skipping bundle generation.")
            return

        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ –ë–î
        all_users = await self.get_all_users_from_db()

        if not all_users:
            print("‚ö†Ô∏è No users found in database. Skipping bundle generation.")
            return

        print(f"üë• Generating bundles for {len(all_users)} users...")

        async with AsyncSessionFactory() as db_session:
            try:
                today = date.today()

                for user_data in all_users:
                    user_id = user_data["id"]
                    user_email = user_data["email"]
                    user_profile = user_data.get("profile")

                    if not user_profile:
                        print(
                            f"‚ö†Ô∏è No profile found for user {user_email} (ID: {user_id}). Skipping."
                        )
                        continue

                    print(
                        f"  üß† Generating bundle for user {user_email} (ID: {user_id})..."
                    )

                    # --- –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ü–µ—Ä–µ–¥–∞–µ–º classified_news_list –Ω–∞–ø—Ä—è–º—É—é ---
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¢–û–ü-7 –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ò–ó –£–ñ–ï –û–ë–†–ê–ë–û–¢–ê–ù–ù–´–• –Ω–æ–≤–æ—Å—Ç–µ–π
                    top_7_articles = self._select_top_articles_for_user(
                        classified_news_list, user_profile
                    )
                    # --- –ö–û–ù–ï–¶ –ö–õ–Æ–ß–ï–í–û–ì–û –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø ---

                    # --- –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫—ç—à–∞ ---
                    # news_bundle - —ç—Ç–æ —Å–ø–∏—Å–æ–∫ ID –Ω–æ–≤–æ—Å—Ç–µ–π –∏–ª–∏ —Å–∞–º–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
                    # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–∏–º —Å–∞–º–∏ –¥–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–µ–π
                    # --- –í–ê–ñ–ù–û: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –≤—Å–µ –Ω—É–∂–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –≤–µ—Ä—Ö–Ω–µ–º —É—Ä–æ–≤–Ω–µ ---
                    prepared_top_7 = []
                    for article in top_7_articles:
                        # –ö–æ–ø–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
                        prepared_article = article.copy()
                        # –ü–æ–ª—É—á–∞–µ–º ai_analysis (–µ—Å–ª–∏ –µ—Å—Ç—å)
                        ai_analysis = article.get("ai_analysis", {})
                        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –¥–∞–Ω–Ω—ã–µ –∏–∑ ai_analysis –Ω–∞ –≤–µ—Ä—Ö–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å
                        prepared_article["relevance_score"] = ai_analysis.get(
                            "relevance_score",
                            prepared_article.get("relevance_score", 0),
                        )
                        prepared_article["confidence"] = ai_analysis.get(
                            "confidence", prepared_article.get("confidence", 0)
                        )
                        prepared_article["ynk_summary"] = ai_analysis.get(
                            "ynk_summary", prepared_article.get("ynk_summary", "N/A")
                        )
                        # –ú–æ–∂–Ω–æ —Ç–∞–∫–∂–µ –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –¥—Ä—É–≥–∏–µ –¥–∞–Ω–Ω—ã–µ, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ

                        prepared_top_7.append(prepared_article)

                    cache_data = {
                        "generated_at": datetime.utcnow().isoformat(),
                        "top_7": prepared_top_7,  # –°–æ–¥–µ—Ä–∂–∏—Ç ID, title, url, category, relevance_score, importance_score, ynk_summary
                    }

                    # –°–æ–∑–¥–∞–µ–º –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å –≤ –∫—ç—à–µ
                    stmt_check = select(UserNewsCache).where(
                        UserNewsCache.user_id == user_id,
                        UserNewsCache.news_date == today,
                    )
                    result = await db_session.execute(stmt_check)
                    existing_cache = result.scalar_one_or_none()

                    if existing_cache:
                        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∑–∞–ø–∏—Å—å
                        existing_cache.news_bundle = cache_data
                        existing_cache.generated_at = datetime.utcnow()
                        print(f"    üîÑ Updated cache for user {user_email}.")
                    else:
                        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
                        new_cache_entry = UserNewsCache(
                            user_id=user_id, news_date=today, news_bundle=cache_data
                        )
                        db_session.add(new_cache_entry)
                        print(f"    ‚úÖ Cached bundle for user {user_email}.")

                await db_session.commit()
                print(
                    f"üéâ All {len(all_users)} user bundles cached successfully for {today}."
                )
            except Exception as e:
                print(f"‚ö†Ô∏è Error generating/caching bundles for users: {e}")
                await db_session.rollback()

    async def generate_and_cache_podcasts_for_premium_users(
        self, classified_news_list: List[Dict]
    ):
        """
        –§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ 3: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–¥–∫–∞—Å—Ç—ã –¥–ª—è –í–°–ï–• –ü–†–ï–ú–ò–£–ú –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –≤ –∫—ç—à (user_news_cache).
        –ò–°–ü–û–õ–¨–ó–£–ï–¢ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ classified_news_list.

        Args:
            classified_news_list: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–æ–≤–æ—Å—Ç–µ–π (–≤–∫–ª—é—á–∞—è ID –∏ YNK).
        """
        print(
            "\n--- [BACKGROUND TASK 3] Generating Personalized Podcasts for ALL Premium Users ---"
        )

        if not classified_news_list:
            print("‚ö†Ô∏è No classified news provided. Skipping podcast generation.")
            return

        if not PODCAST_GENERATOR_AVAILABLE or not self.podcast_generator:
            print("‚ö†Ô∏è Podcast generator is not available. Skipping podcast generation.")
            return

        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ –ë–î
        all_users = await self.get_all_users_from_db()

        if not all_users:
            print("‚ö†Ô∏è No users found in database. Skipping podcast generation.")
            return

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–µ–º–∏—É–º-–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        # TODO: –ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Ä–µ–∞–ª—å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É —Å—Ç–∞—Ç—É—Å–∞ –ø—Ä–µ–º–∏—É–º-–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        # –ù–∞–ø—Ä–∏–º–µ—Ä: is_premium = await check_user_premium_status(user_id, db_session)
        # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É: –≤—Å–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —Å —á–µ—Ç–Ω—ã–º ID —Å—á–∏—Ç–∞—é—Ç—Å—è –ø—Ä–µ–º–∏—É–º
        premium_users = [
            user for user in all_users if user["id"] % 2 == 0
        ]  # <-- –ó–ê–ì–õ–£–®–ö–ê
        # premium_users = [user for user in all_users if user['id'] in [1, 2]] # <-- –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ù–ê–Ø –ó–ê–ì–õ–£–®–ö–ê (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ 1 –∏ 2)
        # premium_users = all_users # <-- –ó–ê–ì–õ–£–®–ö–ê (–≤—Å–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–º–∏—É–º)

        if not premium_users:
            print("‚ö†Ô∏è No premium users found in database. Skipping podcast generation.")
            return

        print(f"üë• Generating podcasts for {len(premium_users)} premium users...")

        async with AsyncSessionFactory() as db_session:
            try:
                today = date.today()

                for user_data in premium_users:
                    user_id = user_data["id"]
                    user_email = user_data["email"]
                    user_profile = user_data.get("profile")

                    if not user_profile:
                        print(
                            f"‚ö†Ô∏è No profile found for premium user {user_email} (ID: {user_id}). Skipping."
                        )
                        continue

                    print(
                        f"  üéôÔ∏è  Generating podcast for premium user {user_email} (ID: {user_id})..."
                    )

                    # --- –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ü–µ—Ä–µ–¥–∞–µ–º classified_news_list –Ω–∞–ø—Ä—è–º—É—é ---
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¢–û–ü-7 –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ò–ó –£–ñ–ï –û–ë–†–ê–ë–û–¢–ê–ù–ù–´–• –Ω–æ–≤–æ—Å—Ç–µ–π
                    top_7_articles = self._select_top_articles_for_user(
                        classified_news_list, user_profile
                    )
                    # --- –ö–û–ù–ï–¶ –ö–õ–Æ–ß–ï–í–û–ì–û –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø ---

                    # --- –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ–¥–∫–∞—Å—Ç–∞ ---
                    # news_bundle - —ç—Ç–æ —Å–ø–∏—Å–æ–∫ ID –Ω–æ–≤–æ—Å—Ç–µ–π –∏–ª–∏ —Å–∞–º–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
                    # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–∏–º —Å–∞–º–∏ –¥–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–µ–π
                    # --- –í–ê–ñ–ù–û: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –≤—Å–µ –Ω—É–∂–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –≤–µ—Ä—Ö–Ω–µ–º —É—Ä–æ–≤–Ω–µ ---
                    prepared_top_7 = []
                    for article in top_7_articles:
                        # –ö–æ–ø–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
                        prepared_article = article.copy()
                        # –ü–æ–ª—É—á–∞–µ–º ai_analysis (–µ—Å–ª–∏ –µ—Å—Ç—å)
                        ai_analysis = article.get("ai_analysis", {})
                        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –¥–∞–Ω–Ω—ã–µ –∏–∑ ai_analysis –Ω–∞ –≤–µ—Ä—Ö–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å
                        prepared_article["relevance_score"] = ai_analysis.get(
                            "relevance_score",
                            prepared_article.get("relevance_score", 0),
                        )
                        prepared_article["confidence"] = ai_analysis.get(
                            "confidence", prepared_article.get("confidence", 0)
                        )
                        prepared_article["ynk_summary"] = ai_analysis.get(
                            "ynk_summary", prepared_article.get("ynk_summary", "N/A")
                        )
                        # –ú–æ–∂–Ω–æ —Ç–∞–∫–∂–µ –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –¥—Ä—É–≥–∏–µ –¥–∞–Ω–Ω—ã–µ, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ

                        prepared_top_7.append(prepared_article)

                    # --- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–¥–∫–∞—Å—Ç–∞ ---
                    try:
                        print(
                            f"    üß† Calling podcast generator for user {user_email}..."
                        )
                        podcast_script = (
                            await self.podcast_generator.generate_podcast_script(
                                user_profile, prepared_top_7
                            )
                        )
                        print(f"    ‚úÖ Podcast script generated for user {user_email}.")
                    except Exception as e:
                        print(
                            f"    ‚ö†Ô∏è Error generating podcast script for user {user_email}: {e}"
                        )
                        podcast_script = "Sorry, the podcast script could not be generated at this time."
                    # --- –ö–û–ù–ï–¶ –ì–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥–∫–∞—Å—Ç–∞ ---

                    # --- –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à —Å –ø–æ–¥–∫–∞—Å—Ç–æ–º ---
                    stmt_check = select(UserNewsCache).where(
                        UserNewsCache.user_id == user_id,
                        UserNewsCache.news_date == today,
                    )
                    result = await db_session.execute(stmt_check)
                    existing_cache = result.scalar_one_or_none()

                    if existing_cache:
                        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∑–∞–ø–∏—Å—å
                        existing_cache.news_bundle["podcast_script"] = podcast_script
                        existing_cache.generated_at = datetime.utcnow()
                        print(
                            f"    üîÑ Updated cache with podcast for user {user_email}."
                        )
                    else:
                        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å —Å –ø–æ–¥–∫–∞—Å—Ç–æ–º
                        cache_data = {
                            "generated_at": datetime.utcnow().isoformat(),
                            "top_7": prepared_top_7,  # –°–æ–¥–µ—Ä–∂–∏—Ç ID, title, url, category, relevance_score, importance_score, ynk_summary
                            "podcast_script": podcast_script,
                        }
                        new_cache_entry = UserNewsCache(
                            user_id=user_id, news_date=today, news_bundle=cache_data
                        )
                        db_session.add(new_cache_entry)
                        print(
                            f"    ‚úÖ Cached bundle with podcast for user {user_email}."
                        )

                await db_session.commit()
                print(
                    f"üéâ All {len(premium_users)} premium user podcasts cached successfully for {today}."
                )
            except Exception as e:
                print(f"‚ö†Ô∏è Error generating/caching podcasts for premium users: {e}")
                await db_session.rollback()

    async def get_cached_news_bundle_for_user(self, user_id: int) -> Optional[Dict]:
        """
        API-–º–µ—Ç–æ–¥: –ü–æ–ª—É—á–∞–µ—Ç –≥–æ—Ç–æ–≤—É—é –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ª–µ–Ω—Ç—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑ –∫—ç—à–∞.

        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –ë–î.

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–æ–º 'top_7' –∏ —Å–ø–∏—Å–∫–æ–º –Ω–æ–≤–æ—Å—Ç–µ–π, –∏–ª–∏ None, –µ—Å–ª–∏ –∫—ç—à –Ω–µ –Ω–∞–π–¥–µ–Ω.
        """
        if not DATABASE_AVAILABLE or not AsyncSessionFactory or not UserNewsCache:
            print("‚ö†Ô∏è Database not configured for fetching cached news.")
            return None

        async with AsyncSessionFactory() as db_session:
            try:
                today = date.today()
                stmt = select(UserNewsCache).where(
                    UserNewsCache.user_id == user_id, UserNewsCache.news_date == today
                )
                result = await db_session.execute(stmt)
                cache_entry = result.scalar_one_or_none()

                if cache_entry:
                    print(f"‚úÖ Found cached news bundle for user ID {user_id}.")
                    # news_bundle - —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å, –∫–æ—Ç–æ—Ä—ã–π –º—ã —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏
                    return cache_entry.news_bundle
                else:
                    print(
                        f"‚ö†Ô∏è No cached news bundle found for user ID {user_id} for {today}."
                    )
                    return None
            except Exception as e:
                print(f"‚ö†Ô∏è Error fetching cached news for user {user_id}: {e}")
                return None

    async def get_cached_podcast_script_for_user(self, user_id: int) -> Optional[Dict]:
        """
        API-–º–µ—Ç–æ–¥: –ü–æ–ª—É—á–∞–µ—Ç –≥–æ—Ç–æ–≤—ã–π –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥–∫–∞—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑ –∫—ç—à–∞.

        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –ë–î.

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–æ–º 'script' –∏ —Ç–µ–∫—Å—Ç–æ–º –ø–æ–¥–∫–∞—Å—Ç–∞, –∏–ª–∏ None, –µ—Å–ª–∏ –∫—ç—à –Ω–µ –Ω–∞–π–¥–µ–Ω.
        """
        if not DATABASE_AVAILABLE or not AsyncSessionFactory or not UserNewsCache:
            print("‚ö†Ô∏è Database not configured for fetching cached podcast.")
            return None

        async with AsyncSessionFactory() as db_session:
            try:
                today = date.today()
                stmt = select(UserNewsCache).where(
                    UserNewsCache.user_id == user_id, UserNewsCache.news_date == today
                )
                result = await db_session.execute(stmt)
                cache_entry = result.scalar_one_or_none()

                if cache_entry and cache_entry.news_bundle:
                    podcast_script = cache_entry.news_bundle.get("podcast_script")
                    if podcast_script:
                        print(f"‚úÖ Found cached podcast script for user ID {user_id}.")
                        return {"script": podcast_script}
                    else:
                        print(
                            f"‚ö†Ô∏è No podcast script found in cache for user ID {user_id} for {today}."
                        )
                        return None
                else:
                    print(
                        f"‚ö†Ô∏è No cached news bundle found for user ID {user_id} for {today}."
                    )
                    return None
            except Exception as e:
                print(f"‚ö†Ô∏è Error fetching cached podcast for user {user_id}: {e}")
                return None

    async def generate_podcast_script_for_user(
        self, user_id: int, top_7_articles: List[Dict]
    ) -> Optional[Dict[str, str]]:
        """
        Generates a personalized podcast script for a user based on their TOP-7 articles.

        Args:
            user_id: The ID of the user in the database.
            top_7_articles: The list of 7 articles from the user's personalized feed.

        Returns:
            A dictionary with the key 'script' and the podcast text, or None on error.
        """
        print(f"üéôÔ∏è  Generating podcast script for user ID: {user_id}...")

        if not top_7_articles:
            print(
                f"‚ö†Ô∏è  No TOP-7 articles provided for user {user_id}. Cannot generate podcast."
            )
            return None

        if not self.podcast_generator:
            print(f"‚ö†Ô∏è  Podcast generator not available for user {user_id}.")
            return None

        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑ –ë–î
            async with AsyncSessionFactory() as db_session:
                stmt = (
                    select(DBUser, DBUserProfile)
                    .join(DBUserProfile, isouter=True)
                    .where(DBUser.id == user_id)
                )
                result = await db_session.execute(stmt)
                db_user_with_profile = result.first()

                if not db_user_with_profile:
                    print(
                        f"‚ö†Ô∏è  User with ID {user_id} not found in DB for podcast generation."
                    )
                    return None

                db_user, db_profile = db_user_with_profile
                user_profile_data = {
                    "user_id": db_user.id,
                    "email": db_user.email,
                    "locale": db_profile.locale if db_profile else "US",
                    "language": "en",  # Default or from profile
                    "city": None,  # Not stored
                    "interests": db_profile.interests if db_profile else [],
                    "is_premium": db_profile.is_premium
                    if db_profile
                    else False,  # <-- –ü–æ–ª—É—á–∞–µ–º is_premium
                }

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∫—Ä–∏–ø—Ç –ø–æ–¥–∫–∞—Å—Ç–∞
            podcast_script = self.podcast_generator.generate_podcast_script(
                user_profile_data, top_7_articles
            )  # <-- await

            print(f"‚úÖ  Podcast script generated successfully for user ID {user_id}.")
            return {"script": podcast_script}

        except Exception as e:
            print(f"‚ùå  Error generating podcast script for user {user_id}: {e}")
            return None

    # --- –ö–û–ù–ï–¶ –ù–û–í–û–ì–û –ú–ï–¢–û–î–ê ---

    # --- –°–¢–ê–†–´–ô –ú–ï–¢–û–î process_daily_news (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –∏–ª–∏ –ø—Ä—è–º–æ–≥–æ –≤—ã–∑–æ–≤–∞) ---
    # –û–Ω —Ç–µ–ø–µ—Ä—å –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–æ–≤—ã–π –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –µ—Å–ª–∏ –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –∏–∑ API
    async def process_daily_news(
        self, user_preferences: Union[Dict, Any]
    ) -> Dict[str, List[Dict]]:
        """
        Process daily news batch for a user.
        NOTE: This method is now primarily for API use and reads from cache.
        For background processing, use fetch_and_classify_all_news and generate_and_cache_bundles_for_all_users.

        Args:
            user_preferences: User profile (object or dict). Used to get user_id for cache lookup.

        Returns:
            Dictionary containing the 'top_7' articles from cache, or an error message.
        """
        # Ensure user_preferences is a dict for internal use
        user_prefs_dict = self._convert_user_profile_to_dict(user_preferences)
        user_id = user_prefs_dict.get("user_id")

        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å user_id –∫–∞–∫ int –∏–∑ –ë–î (–µ—Å–ª–∏ —ç—Ç–æ –æ–±—ä–µ–∫—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)
        # –≠—Ç–æ —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞, –≤ —Ä–µ–∞–ª—å–Ω–æ–º API user_id –±—É–¥–µ—Ç –±—Ä–∞—Ç—å—Å—è –∏–∑ —Ç–æ–∫–µ–Ω–∞
        db_user_id = None
        if isinstance(user_id, int):
            db_user_id = user_id
        elif isinstance(user_id, str) and user_id.isdigit():
            db_user_id = int(user_id)
        else:
            # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –ë–î –ø–æ email –∏–∑ –ø—Ä–æ—Ñ–∏–ª—è
            email = user_prefs_dict.get("email") or f"{user_id}@example.com"  # fallback
            if DATABASE_AVAILABLE and AsyncSessionFactory and DBUser:
                async with AsyncSessionFactory() as db_session:
                    try:
                        stmt = select(DBUser).where(DBUser.email == email)
                        result = await db_session.execute(stmt)
                        db_user = result.scalar_one_or_none()
                        if db_user:
                            db_user_id = db_user.id
                    except Exception as e:
                        print(f"‚ö†Ô∏è Error finding user ID for {email}: {e}")

        if db_user_id is None:
            print(
                f"‚ö†Ô∏è Could not determine database user ID for preferences {user_id}. Cannot fetch from cache."
            )
            return {
                "top_7": [],
                "error": "User not found or ID invalid for cache lookup.",
            }

        print(f"\n--- [API REQUEST] Fetching cached news for user ID: {db_user_id} ---")
        cached_bundle = await self.get_cached_news_bundle_for_user(db_user_id)

        if cached_bundle and "top_7" in cached_bundle:
            print(f"‚úÖ Returned cached TOP-7 for user ID {db_user_id}.")
            return cached_bundle
        else:
            print(
                f"‚ö†Ô∏è No cached news available for user ID {db_user_id}. Please run the background daily pipeline first."
            )
            return {
                "top_7": [],
                "message": "Your personalized news feed is being prepared. Please try again in a few minutes.",
            }

    async def run_full_daily_pipeline(self):
        """
        Runs the NEW complete daily pipeline: fetch, classify, save, personalize, cache.
        This is the main orchestrator for the background task.
        """
        print("üöÄ Starting NEW Full Daily News Pipeline Run (Background Task)...")
        pipeline_start_time = time.time()

        # --- 1. Fetch and classify all news ---
        classified_news = await self.fetch_and_classify_all_news()

        # --- 2. Generate and cache bundles for all users ---
        await self.generate_and_cache_bundles_for_all_users(classified_news)

        # --- 3. Generate and cache podcasts for premium users ---
        await self.generate_and_cache_podcasts_for_premium_users(classified_news)

        # --- 4. Run Data Retention Cleanup ---
        await self._run_data_retention_cleanup()

        # --- 5. Finalize ---
        total_pipeline_time = time.time() - pipeline_start_time
        print(
            f"\nüèÅ NEW Full Daily Pipeline Run Completed in {total_pipeline_time:.1f}s"
        )


# --- –°–∫—Ä–∏–ø—Ç –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ —Ñ–æ–Ω–æ–≤–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ ---
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Run the News Processing Pipeline")
    parser.add_argument(
        "--mode",
        choices=["background", "test-api"],
        default="background",
        help="Run mode: 'background' for full daily job, 'test-api' for single user test",
    )
    parser.add_argument("--user-id", type=int, help="User ID for test-api mode")

    args = parser.parse_args()

    # Initialize pipeline
    pipeline = NewsProcessingPipeline(max_workers=3)

    if args.mode == "background":
        print("\n--- Initiating Full Asynchronous Background Pipeline Run ---")
        # Run the NEW async pipeline
        asyncio.run(pipeline.run_full_daily_pipeline())
        print("--- Full Asynchronous Background Pipeline Run Finished ---")

    elif args.mode == "test-api":
        print("\n--- Initiating Test API Call ---")
        # For testing the API-like behavior (reads from cache)
        if not args.user_id:
            print("Error: --user-id is required for test-api mode.")
            sys.exit(1)

        async def test_api_call():
            sample_user_prefs = {
                "user_id": args.user_id,  # This should be a real DB user ID
                "email": f"user{args.user_id}@example.com",  # Fallback
            }
            result = await pipeline.process_daily_news(sample_user_prefs)
            if "top_7" in result and result["top_7"]:
                print(f"\n--- Test API Result for User {args.user_id} ---")
                for i, article in enumerate(result["top_7"]):
                    print(f"\n--- Article {i+1} ---")
                    print(f"üì∞ Title: {article.get('title')}")
                    print(f"üîó URL: {article.get('url')}")
                    print(f"üè∑Ô∏è  Category: {article.get('category')}")
                    print(f"üìä Relevance Score: {article.get('relevance_score', 'N/A')}")
                    print(
                        f"üìà Importance Score: {article.get('importance_score', 'N/A')}"
                    )
                    ynk = article.get("ynk_summary", "N/A")
                    print(f"üí° YNK Summary: {ynk}")  # –í—ã–≤–æ–¥–∏–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç

                # --- –ù–û–í–û–ï: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥ –ø–æ–¥–∫–∞—Å—Ç–∞ –¥–ª—è –ø—Ä–µ–º–∏—É–º-–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π ---
                # –ü—Ä–æ–≤–µ—Ä–∏–º —Å—Ç–∞—Ç—É—Å –ø—Ä–µ–º–∏—É–º —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (—á–µ—Ä–µ–∑ –ë–î –∏–ª–∏ –≤—Ä–µ–º–µ–Ω–Ω—É—é –∑–∞–≥–ª—É—à–∫—É)
                # –í—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–≥–ª—É—à–∫–∞: –≤—Å–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —Å —á–µ—Ç–Ω—ã–º ID - –ø—Ä–µ–º–∏—É–º
                # TODO: –ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Ä–µ–∞–ª—å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –∏–∑ –ë–î
                is_premium_user = args.user_id % 2 == 0  # <-- TEMPORARY FOR TESTING
                # is_premium_user = True # <-- ALTERNATIVE TEMPORARY FOR TESTING (–≤—Å–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–º–∏—É–º)

                if is_premium_user:
                    print(f"\nüéôÔ∏è  üéß  üé§  üéß  üéôÔ∏è  üéß  üé§  üéß  üéôÔ∏è  üéß  üé§  üéß")
                    print(
                        f"üéß  Generating personalized podcast for PREMIUM user {args.user_id}...  üéß"
                    )
                    print(f"üéôÔ∏è  üéß  üé§  üéß  üéôÔ∏è  üéß  üé§  üéß  üéôÔ∏è  üéß  üé§  üéß")

                    try:
                        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–¥–∫–∞—Å—Ç
                        podcast_result = (
                            await pipeline.generate_podcast_script_for_user(
                                args.user_id, result["top_7"]
                            )
                        )  # <-- await
                        if podcast_result and "script" in podcast_result:
                            print(
                                f"\n--- üéôÔ∏è  Personalized Podcast Script for User {args.user_id} ---"
                            )
                            print(
                                podcast_result["script"]
                            )  # –í—ã–≤–æ–¥–∏–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ–¥–∫–∞—Å—Ç–∞
                            print(f"\n--- üéß End of Podcast Script ---")
                        else:
                            print(
                                f"\n‚ö†Ô∏è  Podcast script could not be generated for user {args.user_id}."
                            )
                    except Exception as e:
                        print(
                            f"\n‚ùå Error generating podcast for user {args.user_id}: {e}"
                        )
                else:
                    print(
                        f"\n‚ÑπÔ∏è  User {args.user_id} is not a premium user. Podcast generation skipped."
                    )
                # --- –ö–û–ù–ï–¶ –ù–û–í–û–ì–û ---
            else:
                print(f"\n--- Test API Result for User {args.user_id} ---")
                print("No cached news found or error occurred.")
                print(result)

        asyncio.run(test_api_call())
        print("--- Test API Call Finished ---")
